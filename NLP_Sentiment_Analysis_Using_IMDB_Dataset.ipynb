{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvylS25mInx0usjnzRD6q2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priontu/IMDB-Sentiment-Analysis-using-Keras-on-Tensorflow/blob/main/NLP_Sentiment_Analysis_Using_IMDB_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7xzrEFWnIAf",
        "outputId": "7ed3781e-d271-4f7c-b3ed-cbc1497b5aef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "{'train': <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>, 'test': <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>, 'unsupervised': <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}\n",
            "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
            "Label: 0 = Negative\n",
            "\n",
            "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
            "Label: 0 = Negative\n",
            "\n",
            "Review: Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Moun ...\n",
            "Label: 0 = Negative\n",
            "\n",
            "Review: This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful perf ...\n",
            "Label: 1 = Positive\n",
            "\n",
            "23\n",
            "13\n",
            "12\n",
            "12000\n",
            "tf.Tensor(\n",
            "[[  23   12   29 ...    0    0    0]\n",
            " [   6   22   71 ...    0    0    0]\n",
            " [4099 6881    1 ...    0    0    0]\n",
            " ...\n",
            " [  23   13  119 ...  332 1047    0]\n",
            " [1757 4101  452 ...    0    0    0]\n",
            " [3365 4392    6 ...    0    0    0]], shape=(32, 62), dtype=int64)\n",
            "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "if IS_COLAB:\n",
        "    %pip install -q -U tensorflow-addons\n",
        "    %pip install -q -U transformers\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "    if IS_KAGGLE:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from collections import Counter\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"nlp\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "\n",
        "def preprocess(X_batch, y_batch):\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "    X_batch = tf.strings.regex_replace(X_batch, rb\" \", b\" \")\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "    X_batch = tf.strings.split(X_batch)\n",
        "    return X_batch.to_tensor(default_value=b\"\"), y_batch\n",
        "\n",
        "\n",
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "print(datasets)\n",
        "\n",
        "for X_batch, y_batch in datasets[\"train\"].batch(4).take(1):\n",
        "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
        "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
        "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
        "        print()\n",
        "\n",
        "# preprocess(X_batch, y_batch)\n",
        "\n",
        "\n",
        "vocabulary = Counter()\n",
        "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
        "    for review in X_batch:\n",
        "        vocabulary.update(list(review.numpy()))\n",
        "\n",
        "\n",
        "vocab_size = 12000\n",
        "truncated_vocabulary = [\n",
        "    word for word, count in vocabulary.most_common()[:vocab_size]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
        "for word in b\"This movie was faaaaaantastic\".split():\n",
        "    print(word_to_id.get(word) or vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 2000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)  \n",
        "\n",
        "\n",
        "\n",
        "def encode_words(X_batch, y_batch):\n",
        "    return table.lookup(X_batch), y_batch\n",
        "\n",
        "train_set = datasets[\"train\"].batch(32).map(preprocess).map(encode_words).prefetch(1)\n",
        "valid_set = datasets[\"test\"].batch(32).map(preprocess).map(encode_words).prefetch(1)\n",
        "\n",
        "\n",
        "for X_batch, y_batch in train_set.take(1):\n",
        "    print(X_batch)\n",
        "    print(y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = unit_size = 128\n",
        "ss = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=5e-1,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.7)\n",
        "# ss=5e-1\n",
        "optimizer = keras.optimizers.SGD(learning_rate=ss)\n",
        "\n",
        "checkpoint_cb=keras.callbacks.ModelCheckpoint('/content/gdrive/MyDrive/Colab Notebooks/IMDB_Sentiment_Analysis_Model.h5', \n",
        "                                              save_best_only=True)\n",
        "earlyStop_cb=keras.callbacks.EarlyStopping(patience=10,\n",
        "                                           restore_best_weights=True)\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, \n",
        "                           embed_size,\n",
        "                           mask_zero=True, # not shown in the book\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.GRU(unit_size, \n",
        "                     return_sequences=True),\n",
        "    # keras.layers.GRU(unit_size, \n",
        "    #                  return_sequences=True),\n",
        "    keras.layers.GRU(unit_size),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", \n",
        "              optimizer=optimizer, \n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(train_set, \n",
        "                    epochs=30, \n",
        "                    # steps_per_epoch = train_size//32,\n",
        "                    # batch_size = unit_size, \n",
        "                    validation_data = valid_set, \n",
        "                    callbacks=[checkpoint_cb, earlyStop_cb])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjwkAWETnODI",
        "outputId": "91f8b466-7b7c-4b36-aaa4-bef943a95579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "782/782 [==============================] - 42s 41ms/step - loss: 0.6947 - accuracy: 0.5074 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
            "Epoch 2/30\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.6937 - accuracy: 0.5106 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.6927 - accuracy: 0.5183 - val_loss: 0.6907 - val_accuracy: 0.5255\n",
            "Epoch 4/30\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.6861 - accuracy: 0.5464 - val_loss: 0.6814 - val_accuracy: 0.5536\n",
            "Epoch 5/30\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.6726 - accuracy: 0.5872 - val_loss: 0.6567 - val_accuracy: 0.6338\n",
            "Epoch 6/30\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.6448 - accuracy: 0.6338 - val_loss: 0.8547 - val_accuracy: 0.5016\n",
            "Epoch 7/30\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.6430 - accuracy: 0.6311 - val_loss: 0.6630 - val_accuracy: 0.6006\n",
            "Epoch 8/30\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.6345 - accuracy: 0.6391 - val_loss: 0.7103 - val_accuracy: 0.5831\n",
            "Epoch 9/30\n",
            "782/782 [==============================] - 20s 25ms/step - loss: 0.5401 - accuracy: 0.7298 - val_loss: 0.5667 - val_accuracy: 0.7002\n",
            "Epoch 10/30\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.4886 - accuracy: 0.7662 - val_loss: 0.5187 - val_accuracy: 0.7400\n",
            "Epoch 11/30\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.4480 - accuracy: 0.7926 - val_loss: 0.5028 - val_accuracy: 0.7535\n",
            "Epoch 12/30\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.4109 - accuracy: 0.8154 - val_loss: 0.5048 - val_accuracy: 0.7592\n",
            "Epoch 13/30\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.3742 - accuracy: 0.8366 - val_loss: 0.5273 - val_accuracy: 0.7589\n",
            "Epoch 14/30\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.3397 - accuracy: 0.8580 - val_loss: 0.5646 - val_accuracy: 0.7581\n",
            "Epoch 15/30\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.3086 - accuracy: 0.8763 - val_loss: 0.5995 - val_accuracy: 0.7576\n",
            "Epoch 16/30\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.2763 - accuracy: 0.8939 - val_loss: 0.6271 - val_accuracy: 0.7577\n",
            "Epoch 17/30\n",
            "782/782 [==============================] - 23s 29ms/step - loss: 0.2422 - accuracy: 0.9092 - val_loss: 0.6912 - val_accuracy: 0.7529\n",
            "Epoch 18/30\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.2134 - accuracy: 0.9238 - val_loss: 0.7472 - val_accuracy: 0.7457\n",
            "Epoch 19/30\n",
            "782/782 [==============================] - 20s 25ms/step - loss: 0.1832 - accuracy: 0.9360 - val_loss: 0.7953 - val_accuracy: 0.7451\n",
            "Epoch 20/30\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.1541 - accuracy: 0.9464 - val_loss: 0.8769 - val_accuracy: 0.7418\n",
            "Epoch 21/30\n",
            "782/782 [==============================] - 27s 34ms/step - loss: 0.1261 - accuracy: 0.9582 - val_loss: 1.0504 - val_accuracy: 0.7358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(valid_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg1GCgDTnUHR",
        "outputId": "256ec101-e638-44b1-e266-e78de869d8e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 6s 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.30873862],\n",
              "       [0.55539787],\n",
              "       [0.31562197],\n",
              "       ...,\n",
              "       [0.03534307],\n",
              "       [0.46647537],\n",
              "       [0.7341521 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets['unsupervised']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjsXzzJRnWz2",
        "outputId": "8fcc371d-af2a-43ea-cf2b-ad39fc750356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_size, test_size"
      ],
      "metadata": {
        "id": "a7BeysqUnak6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/IMDB_Sentiment_Analysis_Model.h5\n"
      ],
      "metadata": {
        "id": "v-Icqbs9ndHh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.getcwd()\n"
      ],
      "metadata": {
        "id": "7p_3okR9ndo6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Hwg7tihngOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eQ6HPcI3nh8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# vocabulary.most_common()[:3]\n"
      ],
      "metadata": {
        "id": "iM58dBeKnj1c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(vocabulary)\n"
      ],
      "metadata": {
        "id": "sD2tbDESnqNo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gwsoJ6XNnr_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "_ssdIzT9ntjt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ZXIZrWcknxQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))\n"
      ],
      "metadata": {
        "id": "lRX2_wZEnyzI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# print(train_set.shape)"
      ],
      "metadata": {
        "id": "ktTwg4Bqn0eA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_set"
      ],
      "metadata": {
        "id": "IZ07gJg_n2CN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i3c75pKCn3xy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embed_size = 128\n",
        "# model = keras.models.Sequential([\n",
        "#     keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
        "#                            mask_zero=True, # not shown in the book\n",
        "#                            input_shape=[None]),\n",
        "#     keras.layers.GRU(128, return_sequences=True),\n",
        "#     keras.layers.GRU(128),\n",
        "#     keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "# ])\n",
        "# model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "# history = model.fit(train_set, epochs=5, validation_data=valid_set)\n"
      ],
      "metadata": {
        "id": "P072bHOk9ABZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mp36wsaan5wh"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}